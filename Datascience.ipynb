{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Exemplo Prática Ciência de Dados",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### A Administração de Seguridade Social dos Estados Unidos (SSA) disponibilizou dados sobre a frequência de nomes de bebês de 1880 até o presente. Neste exemplo, vamos usar de \n\n#### Há muitas coisas que você pode querer fazer com o conjunto de dados:\n#### • Visualize a proporção de bebês com um nome específico (o seu próprio ou outro nome) ao longo do tempo\n#### • Determinar a classificação relativa de um nome\n#### • Determine os nomes mais populares em cada ano ou os nomes cuja popularidade avançou ou declinou mais\n#### • Analisar tendências em nomes: vogais, consoantes, comprimento, diversidade geral, mudanças no ortografia, primeiras e últimas letras\n#### • Analisar fontes externas de tendências: nomes bíblicos, celebridades, dados demográficos mudanças",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "url1 = \"https://raw.githubusercontent.com/rlrocha90/IAeML-Treinamento/main/names1.csv\"\nurl2 = \"https://raw.githubusercontent.com/rlrocha90/IAeML-Treinamento/main/names2.csv\"\nurl3 = \"https://raw.githubusercontent.com/rlrocha90/IAeML-Treinamento/main/names3.csv\"\nurl4 = \"https://raw.githubusercontent.com/rlrocha90/IAeML-Treinamento/main/names4.csv\"\nnames1 = pd.read_csv(url1)\nnames2 = pd.read_csv(url2) \nnames3 = pd.read_csv(url3)\nnames4 = pd.read_csv(url4)\nnames = []\nnames.append(names1)\nnames.append(names2)\nnames.append(names3)\nnames.append(names4)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "ename": "<class 'urllib.error.URLError'>",
          "evalue": "<urlopen error unknown url type: https>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m url3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/rlrocha90/IAeML-Treinamento/main/names3.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m url4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/rlrocha90/IAeML-Treinamento/main/names4.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m names1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m names2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url2) \n\u001b[1;32m      7\u001b[0m names3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url3)\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/io/common.py:670\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    667\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 670\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    679\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/io/common.py:339\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    338\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    340\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
            "File \u001b[0;32m/lib/python3.10/site-packages/pandas/io/common.py:239\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/lib/python3.10/urllib/request.py:541\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munknown_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/lib/python3.10/urllib/request.py:1419\u001b[0m, in \u001b[0;36mUnknownHandler.unknown_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munknown_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m-> 1419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown url type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m)\n",
            "\u001b[0;31mURLError\u001b[0m: <urlopen error unknown url type: https>"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "print(names)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Com esses dados em mãos, já podemos começar a agregar os dados no nível de ano e sexo usando groupby ou pivot_table\n#### PivotTable para agregar (somar) os nascimentos por ano e sexo",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "total_births = names.pivot_table(\"births\", index=\"year\", columns=\"sex\", aggfunc=sum)\ntotal_births.plot(title='Total de Nascimentos por sexo e ano')\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Em seguida, vamos inserir um suporte de coluna com a fração de bebês com cada nome em relação ao número\n#### total de nascimentos. Um valor prop de 0,02 indicaria que 2 em cada 100 bebês receberam um nome específico.\n#### Assim, agrupamos os dados por ano e sexo e adicionamos a nova coluna a cada grupo:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def add_prop(group):\n    group[\"prop\"] = group[\"births\"] / group[\"births\"].sum()\n    return group\n\nnames = names.groupby([\"year\", \"sex\"]).apply(add_prop)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### O conjunto de dados completo resultante agora tem as seguintes colunas:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(names)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Ao realizar uma operação de grupo como esta, geralmente é valioso fazer uma verificação de sanidade,\n#### como verificar se a coluna prop soma 1 em todos os grupos:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(names.groupby([\"year\", \"sex\"])[\"prop\"].sum())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Agora que isso está feito, vou extrair um subconjunto dos dados para facilitar uma análise mais aprofundada:\n#### os 1.000 principais nomes para cada combinação de sexo/ano. Esta é mais uma operação de grupo:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_top1000(group):\n    return group.sort_values(\"births\", ascending=False)[:1000]\n\ngrouped = names.groupby([\"year\", \"sex\"])\ntop1000 = grouped.apply(get_top1000)\ntop1000 = top1000.reset_index(drop=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### O conjunto de dados resultante agora é um pouco menor:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(top1000.head())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Analisando tendências de nomes\n#### Com o conjunto de dados completo e o conjunto de dados Top 1.000 em mãos, podemos começar\n#### a analisar várias tendências de nomes de interesse. Dividir os 1.000 principais nomes nas partes de menino e menina é fácil de fazer primeiro:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "boys = top1000[top1000[\"sex\"] == \"M\"]\ngirls = top1000[top1000[\"sex\"] == \"F\"]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Séries temporais simples, como o número de Johns ou Marys para cada ano, podem ser plotadas, mas requerem\n#### um pouco de atenção para serem mais úteis. Vamos formar uma tabela dinâmica do número total de nascimentos por ano e nome:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "total_births = top1000.pivot_table(\"births\", index=\"year\", columns=\"name\", aggfunc=sum)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Agora, isso pode ser plotado para um punhado de nomes com o método de plotagem do DataFrame",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"---- informações do Total de Nascimento ----\")\nprint(total_births.info())\nsubset = total_births[[\"Anna\", \"Sarah\", \"Minnie\", \"Marilyn\"]]\nsubset.plot(subplots=True, figsize=(12, 10), title=\"Número de Nascimentos por ano\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Ao olhar para isso, você pode concluir que esses nomes cresceram em desuso com a população americana. Mas a história é realmente mais complicada do que isso, como será explorado na próxima seção.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Medindo o aumento na diversidade de nomes\n#### Uma explicação para a diminuição das parcelas é que menos pais estão escolhendo nomes comuns para seus filhos.\n#### Essa hipótese pode ser explorada e confirmada nos dados. Uma medida é a proporção de nascimentos representados pelos 1.000 nomes mais populares, que eu agrego e gravo por ano e sexo",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "table = top1000.pivot_table(\"prop\", index=\"year\", columns=\"sex\", aggfunc=sum)\ntable.plot(title=\"Soma de table1000.prop por ano e sexo\", yticks=np.linspace(0, 1.2, 13))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Você pode ver que, de fato, parece estar aumentando a diversidade de nomes (diminuindo a proporção total no top 1.000). Outra métrica interessante é o número de nomes distintos, em ordem de popularidade, do mais alto para o mais baixo, nos 50% melhores nascimentos. Este número é um pouco mais complicado de calcular. Vamos considerar apenas os nomes de meninos de 2010:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = boys[boys[\"year\"] == 2010]\nprint(df)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Depois de classificar prop em ordem decrescente, queremos saber quantos dos nomes mais populares são necessários para chegar a 50%. Você poderia escrever um loop for para fazer isso, mas uma maneira NumPy vetorizada é um pouco mais inteligente. Pegar a soma cumulativa, cumsum, de prop e então chamar o método searchsorted retorna a posição na soma cumulativa na qual 0.5 precisaria ser inserido para mantê-la em ordem:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "prop_cumsum = df[\"prop\"].sort_values(ascending=False).cumsum()\nprint(prop_cumsum[0:10])\nplt.hist(prop_cumsum)\nplt.show()\nprop_cumsum.plot.density()\nplt.show()\nprint(prop_cumsum.searchsorted(0.5))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Como as matrizes são indexadas a zero, adicionar 1 a esse resultado resulta em 117. Em contraste, em 1951 esse número era muito menor:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = boys[boys.year == 1951]\nin19510 = df.sort_values(\"prop\", ascending=False).prop.cumsum()\nprint(in1951.searchsorted(0.5) + 1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Agora você pode aplicar esta operação a cada combinação de ano/sexo, agrupar por esses campos e aplicar uma função que retorna a contagem para cada grupo:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_quantile_count(group, q=0.5):\n    group = group.sort_values(\"prop\", ascending=False)\n    return group.prop.cumsum().searchsorted(q) + 1\n\ndiversity = top1000.groupby([\"year\", \"sex\"]).apply(get_quantile_count)\ndiversity = diversity.unstack()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Essa diversidade de DataFrame resultante agora tem duas séries temporais, uma para cada sexo, indexadas por ano.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(diversity.head())\ndiversity.plot(title=\"Número de nomes populares no top 50%\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Como você pode ver, os nomes das meninas sempre foram mais diversos do que os nomes dos meninos, e eles só se tornaram mais ao longo do tempo. Uma análise mais aprofundada do que exatamente está impulsionando a diversidade, como o aumento de grafias alternativas, é deixada para o leitor.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### A revolução da “última letra”\n#### Em 2007, a pesquisadora de nomes de bebês Laura Wattenberg apontou em seu site que a distribuição de nomes de meninos por letra final mudou significativamente nos últimos 100 anos. Para ver isso, primeiro agregamos todos os nascimentos no conjunto de dados completo por ano, sexo e letra final:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_last_letter(x):\n    return x[-1]\n\nlast_letters = names[\"name\"].map(get_last_letter)\nlast_letters.name = \"last_letter\"\n\ntable = names.pivot_table(\"births\", index=last_letters, columns=[\"sex\", \"year\"], aggfunc=sum)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Em seguida, selecionamos três anos representativos abrangendo o histórico e imprimimos as primeiras linhas:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "subtable = table.reindex(columns=[1951, 1960, 2010, 2020], level=\"year\")\nprint(subtable.head())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Em seguida, normalize a tabela pelo total de nascimentos para calcular uma nova tabela contendo a proporção do total de nascimentos para cada sexo terminando em cada letra:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(subtable.sum())\nletter_prop = subtable / subtable.sum()\nprint(letter_prop)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Com as proporções das letras agora em mãos, podemos fazer gráficos de barras para cada sexo divididos por ano",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\nletter_prop[\"M\"].plot(kind=\"bar\", rot=0, ax=axes[0], title=\"Masculino\")\nletter_prop[\"F\"].plot(kind=\"bar\", rot=0, ax=axes[1], title=\"Feminino\",legend=False)\nplt.subplots_adjust(hspace=0.25)\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Como você pode ver, os nomes de meninos terminados em n tiveram um crescimento significativo desde a década de 1950. Voltando à tabela completa criada anteriormente, normalizo novamente por ano e sexo e seleciono um subconjunto de letras para os nomes dos meninos, finalmente transpondo para tornar cada coluna uma série temporal:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "letter_prop = table / table.sum()\n\ndny_ts = letter_prop.loc[[\"t\", \"n\", \"y\"], \"M\"].T\nprint(dny_ts.head())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Com este DataFrame de séries temporais em mãos, posso fazer um gráfico das tendências ao longo do tempo novamente com seu método de plotagem",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dny_ts.plot()\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Nomes de meninos que se tornaram nomes de meninas (e vice-versa)\n#### Outra tendência divertida é olhar para nomes de meninos que eram mais populares com um sexo no início da amostra, mas que “mudaram de sexo” no presente. Um exemplo é o nome Lesley ou Leslie. Voltando ao top1000",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "all_names = pd.Series(top1000[\"name\"].unique())\nlesley_like = all_names[all_names.str.contains(\"Lesl\")]\nprint(lesley_like)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### A partir daí, podemos filtrar apenas esses nomes e somar nascimentos agrupados por nome para ver as frequências relativas:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "filtered = top1000[top1000[\"name\"].isin(lesley_like)]\nprint(filtered.groupby(\"name\")[\"births\"].sum())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Em seguida, vamos agregar por sexo e ano e normalizar dentro do ano:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "table = filtered.pivot_table(\"births\", index=\"year\", columns=\"sex\", aggfunc=\"sum\")\ntable = table.div(table.sum(axis=\"columns\"), axis=\"index\")\ntable.tail()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Por fim, agora é possível fazer um gráfico da divisão por sexo ao longo do tempo",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "table.plot(style={\"M\": \"k-\", \"F\": \"k--\"})\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}